{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification\n",
    "Suppose we have a 5-way multi-class classification task. \n",
    "Here are different ways to achieve the same cross entropy function given raw logit output by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size 3, output size 5\n",
    "logit = torch.FloatTensor(([[ 0.0400, -0.0112,  0.0376,  0.0343,  0.6097],\n",
    "        [-1.0835, -0.7568,  0.3562, -0.5957,  0.9419],\n",
    "        [ 1.6908,  1.4120,  0.5687, -0.0944, -1.3327]]))\n",
    "targets = torch.LongTensor([0,1,4])\n",
    "one_hot_vector = torch.LongTensor([[1,0,0,0,0],[0,1,0,0,0],[0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 4])\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)\n",
    "print(one_hot_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6774)\n"
     ]
    }
   ],
   "source": [
    "ce_loss = torch.nn.CrossEntropyLoss()(logit, targets)\n",
    "print(ce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: LogSoftmax + NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6774)\n"
     ]
    }
   ],
   "source": [
    "log_prob = torch.nn.LogSoftmax(dim=1)(logit)\n",
    "logsoftmax_nll_loss = torch.nn.NLLLoss()(log_prob, targets)\n",
    "print(logsoftmax_nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3: Softmax + Log + NLL\n",
    "Note that this is discouraged by pytorch since Softmax can produce NaN when some logit are very close to 0. Try to use LogSoftmax instead of Softmax whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6774)\n"
     ]
    }
   ],
   "source": [
    "prob = torch.nn.Softmax(dim=1)(logit)\n",
    "log_prob = torch.log(prob)\n",
    "softmax_log_nll_loss = torch.nn.NLLLoss()(log_prob, targets)\n",
    "print(softmax_log_nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4: LogSoftmax + my NLL with hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6774)\n"
     ]
    }
   ],
   "source": [
    "log_prob = torch.nn.LogSoftmax(dim=1)(logit)\n",
    "logsoftmax_nll_one_hot_loss = -(log_prob * one_hot_vector).sum(dim=1).mean()\n",
    "print(logsoftmax_nll_one_hot_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Feedback classification\n",
    "Now suppose we are in the LECO setup.\n",
    "In Time 0, we have a binary classification task.\n",
    "In Time 1, we have 5-way classification, and classes [0,1,2] belongs to class 0 in Time 0, and classes [3,4] belongs to class 1 in Time 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit is the same:\n",
      "tensor([[ 0.0400, -0.0112,  0.0376,  0.0343,  0.6097],\n",
      "        [-1.0835, -0.7568,  0.3562, -0.5957,  0.9419],\n",
      "        [ 1.6908,  1.4120,  0.5687, -0.0944, -1.3327]])\n",
      "Each target now has ground truth for both time 0 and time 1:\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [1, 4]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logit is the same:\")\n",
    "print(logit)\n",
    "targets = torch.LongTensor([[0, 0], [0, 1], [1, 4]])\n",
    "print(\"Each target now has ground truth for both time 0 and time 1:\")\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Please skip the below cell) Helper functions I implemented to create one hot vector from targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot vector for targets is:\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# dict[lead_idx] = targets\n",
    "leaf_idx_to_all_class_idx = {\n",
    "    0 : [0, 0],\n",
    "    1 : [0, 1],\n",
    "    2 : [0, 2],\n",
    "    3 : [1, 3],\n",
    "    4 : [1, 4]\n",
    "}\n",
    "\n",
    "def get_superclass_to_subclass(leaf_idx_to_all_class_idx):\n",
    "    # superclass_to_subclass[sub_class_time][super_class_time][super_class_idx]\n",
    "    # is the set of indices in sub_class_time that correspond to the superclass\n",
    "    num_of_levels = len(leaf_idx_to_all_class_idx[list(leaf_idx_to_all_class_idx.keys())[0]])\n",
    "    superclass_to_subclass = {}\n",
    "    for tp_idx in range(num_of_levels-1, -1, -1):\n",
    "        superclass_to_subclass[tp_idx] = {}\n",
    "        for super_class_time in range(tp_idx+1):\n",
    "            superclass_to_subclass[tp_idx][super_class_time] = {}\n",
    "            for leaf_idx in leaf_idx_to_all_class_idx:\n",
    "                sub_class_idx = leaf_idx_to_all_class_idx[leaf_idx][tp_idx]\n",
    "                super_class_idx = leaf_idx_to_all_class_idx[leaf_idx][super_class_time]\n",
    "                if not super_class_idx in superclass_to_subclass[tp_idx][super_class_time]:\n",
    "                    superclass_to_subclass[tp_idx][super_class_time][super_class_idx] = [sub_class_idx]\n",
    "                elif not sub_class_idx in superclass_to_subclass[tp_idx][super_class_time][super_class_idx]:\n",
    "                    superclass_to_subclass[tp_idx][super_class_time][super_class_idx].append(sub_class_idx)\n",
    "    return superclass_to_subclass\n",
    "\n",
    "# superclass_to_subclass[sub_class_time][super_class_time][super_class_idx]\n",
    "# is the set of indices in sub_class_time that correspond to the superclass\n",
    "superclass_to_subclass = get_superclass_to_subclass(leaf_idx_to_all_class_idx)\n",
    "\n",
    "# print(superclass_to_subclass)\n",
    "\n",
    "num_of_classes = [2, 5]\n",
    "\n",
    "def get_make_hot_vector_func(superclass_to_subclass,\n",
    "                             num_of_classes,\n",
    "                             tp_idx):\n",
    "    # superclass_to_subclass[tp_idx][super_class_time][super_class_idx] is the set\n",
    "    # of indices (in current_time:tp_idx) included in the superclass\n",
    "\n",
    "    # Return a function that makes a one hot vector from timestamp and labels\n",
    "    num_of_leaf_classes = num_of_classes[tp_idx]\n",
    "    def make_hot_vector(time_indices, labels, device='cuda'):\n",
    "        hot_vector = torch.zeros((time_indices.shape[0], num_of_leaf_classes)).to(device)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        for idx, super_class_time in enumerate(time_indices):\n",
    "            super_class_idx = int(labels[int(super_class_time)][idx])\n",
    "            # if super_class_time < tp_idx:\n",
    "            label_indices = superclass_to_subclass[tp_idx][int(super_class_time)][super_class_idx]\n",
    "            hot_vector[idx, label_indices] = 1\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # elif super_class_time == tp_idx:\n",
    "            #     hot_vector[idx, label_idx] = 1\n",
    "            # else:\n",
    "            #     raise ValueError('Invalid time index')\n",
    "        return hot_vector\n",
    "    return make_hot_vector\n",
    "\n",
    "hot_vector_func = get_make_hot_vector_func(superclass_to_subclass,\n",
    "                             num_of_classes, 1)\n",
    "\n",
    "def get_labels(targets):\n",
    "    labels = []\n",
    "    for i in range(len(targets[0])):\n",
    "        labels.append([t[i] for t in targets])\n",
    "    return labels\n",
    "labels = get_labels(targets)\n",
    "\n",
    "hot_vector = hot_vector_func(torch.LongTensor([1,1,1]), labels, device='cpu')\n",
    "print(\"Hot vector for targets is:\")\n",
    "print(hot_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 5: LogSoftmax + my NLL with hot vector generated by helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6774)\n"
     ]
    }
   ],
   "source": [
    "log_prob = torch.nn.LogSoftmax(dim=1)(logit)\n",
    "logsoftmax_nll_one_hot_loss = -(log_prob * hot_vector).sum(dim=1).mean()\n",
    "print(logsoftmax_nll_one_hot_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Feedback Loss\n",
    "Now suppose we are working with partial feedback with history samples. In the above example, now let's assume the 0th item is history sample. Here would be it's hot vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot vector for targets is:\n",
      "tensor([[1., 1., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "hot_vector = hot_vector_func(torch.LongTensor([0,1,1]), labels, device='cpu')\n",
    "print(\"Hot vector for targets is:\")\n",
    "print(hot_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Feedback (My loss, not Peiyun's):\n",
    "My partial feedback loss first calculate the log prob (via log softmax), then sum the logged probability for negative likelihood.\n",
    "My loss is an upper bound of Peiyun's loss and therefore should serve as a better loss surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8567)\n"
     ]
    }
   ],
   "source": [
    "log_prob = torch.nn.LogSoftmax(dim=1)(logit)\n",
    "my_loss = -(log_prob * hot_vector).sum(dim=1).mean()\n",
    "print(my_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we could use softmax instead of logsoftmax, though softmax is relatively unstable to use, and will certainly produce NaN in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8567)\n"
     ]
    }
   ],
   "source": [
    "log_prob = torch.log(torch.nn.Softmax(dim=1)(logit))\n",
    "loss = -(log_prob * hot_vector).sum(dim=1)\n",
    "# print(loss)\n",
    "my_loss = loss.mean()\n",
    "print(my_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Feedback (Peiyun's loss):\n",
    "Peiyun's loss first calculate the prob (via softmax), then sum the probability, and send the total sum to log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3171)\n"
     ]
    }
   ],
   "source": [
    "prob = torch.nn.Softmax(dim=1)(logit)\n",
    "prob_mask = (prob * hot_vector).sum(dim=1)\n",
    "# print(prob_mask)\n",
    "loss = -torch.log(prob_mask)\n",
    "# print(loss)\n",
    "peiyun_loss = loss.mean()\n",
    "print(peiyun_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my alternative peiyun's loss instantiation to compute the same above loss function by shifting the logit (by max value) before sending into Softmax, and therefore more numerically stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3171)\n"
     ]
    }
   ],
   "source": [
    "max_logit = logit.max(1)[0].unsqueeze(1)\n",
    "shifted_logit = logit - max_logit\n",
    "prob = torch.nn.Softmax(dim=1)(shifted_logit)\n",
    "prob_mask = (prob * hot_vector).sum(dim=1)\n",
    "loss = -torch.log(prob_mask)\n",
    "peiyun_loss = loss.mean()\n",
    "print(peiyun_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
